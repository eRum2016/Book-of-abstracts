\providecommand{\main}{..} 
\documentclass[\main/boa.tex]{subfiles}

\begin{document}
\pagestyle{empty}

\section{Using SparkR with distributed database in Parquet - a genomic example}

\begin{center}
  {\bf \index[a]{Okoniewski Michał} Michał Okoniewski$^{1^\star}$}
\end{center}

\vskip 0.3cm

\begin{affiliations}
\begin{enumerate}
\begin{minipage}{0.915\textwidth}
\centering
\item Scientific IT Services, ETH Zurich \\[-2pt]
\end{minipage}
\end{enumerate}
$^\star$Contact author: \href{mailto:michal.okoniewski@id.ethz.ch}{\nolinkurl{michal.okoniewski@id.ethz.ch}}\\
\end{affiliations}

\vskip 0.5cm

\begin{minipage}{0.915\textwidth}
\keywords SparkR; next-generation sequencing; ADAM
\packages \index[p]{SparkR}
\end{minipage}

\vskip 0.8cm

\emph{SparkR} is the \textbf{R} interface to Apache Spark map-reduce
framework. It does not have the full capabilities the three main
languages of Spark: scala, java and python, still can be used as an
interface between scalable processing of distributed data frames and the
vast galaxy of \emph{R} data analytics.

A good example of a system where \emph{SparkR} can be used to provide
data to \textbf{R} analytical front-end is ADAM. It is a set of formats
and Spark operations to process large next-generation sequencing
datasets. ADAM keeps the data in Parquet columnar storage, which can be
queried using SQL. This provides \textbf{R} analytics with a fully
scalable data back-end. Overall, the presentation is a proof that it is
possible to process real big data with it - not only in genomics, but in
various other data science applications. It will include practical hints
on using \emph{SparkR} in Jupyter notebooks with Spark data frames and
Parquet storage.

\end{document}
